# 텍스트 처리 지식 없이 잠정 3위까지 올라가기 그리고 최종 탈락

## :whale: 순서
* [팀 소개](#whale-팀-소개)
* [참가 계기](#whale-참가-계기)
* [진행 과정 및 문제 접근 방식](#whale-진행-과정-및-문제-접근-방식)
  * 전체적인 진행 과정
  * 베이스라인 모델 요약
  * 텍스트 데이터 처리
  * 네트워크 구조
  * 라벨이 없는(라벨이 -1인) 데이터 처리
  * 앙상블 처리
* [스코어 개선 과정 요약](#whale-스코어-개선-과정-요약)
* [후기](#whale-후기)
* [대회 또는 서비스의 개선 요구 사항](#whale-대회-또는-서비스의-개선-요구-사항)

## :whale: 팀 소개
* 안녕하세요:hamster: 카카오 쇼핑 카테고리 분류 대회에 참가한 **S2Bstar** 팀입니다. 이번 대회에서 TEST 스코어로 3위(잠정 3위)를 했었지만, 입상 조건을 충족하지 못하여 최종 탈락하게되었습니다. 

* S2Bstar의 팀원은 총 셋입니다. 한분은 작은 스타트업에서 데이터 분석 일을 맡고 있습니다. 다른 한분은 포트란 사용자로 대기과학관련 박사과정 중입니다. 또다른 한 분은 물리학과 학부생 2학년이고 곧 군대에 들어간다고 합니다. 서로 다른 계기에 의해 데이터 분석 분야에 관심을 갖게 되었고, 2018년 초에 팀을 형성해서 지속적으로 캐글 대회를 중심으로 참여하며 데이터 분석을 해왔습니다. 

* 최종 결과가 조금 아쉬웠지만 팀원들과 정말 즐겁게 대회에 참여하고 많은 것을 배웠습니다. 특히, 자연어 처리에 대한 지식이 모두 전무한 상태로 시작했지만, **반복적인 데이터 분석**을 바탕으로 점수를 지속적으로 높여갔습니다. 이 글에서는 저희 팀의 **수행과정**, **시도했던 아이디어들** 그리고 **최종 탈락 이유**에 대해서 다루어보려고 합니다.


## :whale: 참가 계기
* 저희 팀은 데이터 분석 대회에 참여하여 재미있게 즐기며 실력을 기르는 것이 목표입니다. 마침 카카오 아레나에서 쇼핑 카테고리 분류 대회를 개최한다고 하여 관심을 가지고 참여하게 되었습니다. 특히, 저희는 텍스트 데이터에 대한 처리 경험이 전무하였기에, 이번 기회를 통해 텍스트 데이터 처리에 대해 많이 배우고 생각해보고자 참여하게 되었습니다.


## :whale: 진행 과정 및 문제 접근 방식

### :octopus::octopus: 전체적인 진행 과정
<p align="center"><img src="/img/fig1.png" width="700"></p>  

* 11월 - 기본적인 네트워크 튜닝 및 형태소 분석기 적용을 바탕으로 빠르게 점수를 올렸습니다.   
* 12월 - 토큰화, 임베딩 그리고 네트워크에 대한 실험을 굉장히 많이 했지만, 퍼블릭 스코어가 하나도 오르지 않는 **기적의 정체기**였습니다. 
* 01월 - 대회 종료일이 얼마남지 않은 1월부터는 여러 실험 끝에 다시 스코어가 크게 오르게 되었습니다.   

### :octopus::octopus: 베이스라인 모델 요약
* 저희 팀은 카카오에서 제공한 베이스라인 모델을 바탕으로 모델을 개선해나갔습니다. 따라서 저희 모델에 대한 내용에 앞서 간단하게 베이스라인 모델에 대해 얘기해보고자 합니다. 

* 베이스라인 모델의 구성은 다음과 같습니다. 텍스트 데이터 처리를 위해 정규표현식을 이용하여 특수문자를 공백으로 치환하고, 공백을 기준으로 문장을 토큰화 합니다. 하나의 토큰(단어)은 해시함수를 통해 유한한 길이의 양의 정수에 대응되고, 이는 다시 고정된 길이의 학습가능한 벡터로 대응(keras.layers.embedding)됩니다. 한 문장에 존재하는 모든 단어 벡터를 합하여(중복된 단어 허용) 해당 문장에 대응되는 고정된 길이의 벡터를 생성합니다. 이렇게 생성된 문장 벡터는 분류기 모델의 입력으로 사용됩니다. 

* 베이스라인 모델의 분류기는 fully-connected layer 들로 구성된 뉴럴 네트워크이며, 출력 레이너는 가능한 '대/중/소/세'  조합의 수인 4215개의 뉴런을 가집니다. 이를 통해 해당 모델은 Multi-class classification 을 수행합니다.  
  
  
### :octopus::octopus: 텍스트 데이터 처리 :sushi: :sushi:
팀원 모두 텍스트 데이터에대한 처리가 처음이다보니, 가장 많은 노력과 시간을 이 과정에 쏟아부었습니다. 하지만 가장 성과가 나지 않은 부분입니다. 시도했던 여러 가지 적용 중에서 높은 개선 효과가 있었던 두 가지 처리 과정에 대해 말해보려고 합니다.

#### - 형태소 분석기 적용 :sushi:
* 베이스라인 모델의 토큰화 성능을 높이기 위해 가장 먼저 한 것은 잘 알려진 한국어 형태소 분석기를 적용하는 것 이었습니다. 사용한 형태소 분석기는 khaiii, kkma, okt 등의 라이브러리를 사용하였습니다. 한 문장에 대해 세 가지 형태소 분석기를 적용하여 명사에 해당하는 단어만 가지고 토큰화를 진행하였고, 해당 토큰들은 모두 하나의 리스트로 합쳤습니다. 각 토크나이저 마다 강점이 다르기에 **세 가지 토크나이저 모두를 사용**하였고, 하나의 토크나이저를 사용한 것보다 개선된 점수를 보여주었습니다. 이때 사용된 입력 문장은 product, brand, maker, maker 의 텍스트가 하나의 텍스트로 이어붙여서 구성되었습니다.

#### - 카테고리 명 저장 (소/세 카테고리 단어 저장) :sushi:
* 형태소 분석기 적용 후 성능이 개선되기는 했지만, 대/중 카테고리에 비해 소/세 카테고리의 정확도는 상대적으로 낮았습니다. 소/세 카테고리를 정확히 맞추지 못한 데이터의 카테고리를 살펴보았더니 **'3.1채널',  '5.1채널', '1단계 분유'**, 등등과 같은 경우를 잘 맞추지 못했습니다. '3.1', '5.1' 같은 숫자들은 소/세 카테고리를 구분할 때 중요한 기준이 되는데, 입력 문장을 살펴보면 tokenizing 과정에서 이러한 숫자 들이 제거되었음을 알게 되었습니다. 

* 어떻게 하면 이 문제를 일반화하여 처리할 수 있을까 고민하다 보니 카테고리 명을 활용하면 되겠다는 생각이 들었습니다. cate1.json에는 카테고리 명들이 나열되어 있기 때문에 모두 핵심단어들이라고 생각했습니다. 그래서 저희 팀은 **cate1.json 에 있는 단어 또는 숫자조합이라면 무조건 코퍼스에 포함시키는 전처리 과정**을 적용했습니다.

* 모든 카테고리 명을 띄어쓰기와 '/' 를 기준으로 토큰화 한것과 Kkma 를 이용해 토큰화를 모두 합쳐서 토큰화된 카테고리명을 저장하였습니다. 그리고, 텍스트 데이터 처리 전에 주어진 문장에서 토큰화된 카테고리명과 동일한 토큰을 따로 기록하였다가, 네트워크의 입력에 추가하였습니다.

  
### :octopus::octopus: 네트워크 구조 :dango::dango:
다양한 구조를 실험해보았지만, 최종적으로 사용된 네트워크 구조는 베이스라인 모델과 거의 동일합니다. 임베딩 레이어및 그 외의 레이어의 뉴런의 수를 조정하고, sigmoid 뉴런을 relu 로 치환하고, Soft-max layer 를 추가하였습니다. 또한 입력으로 img_feat 을 추가하였습니다.

#### - 임베딩 및 추론 네트워크 :dango:
* 저희는 대회 진행 중 효율적인 실험을 위해 네트워크를 임베딩 레이어와 그외의 추론 과정을 분리하여 네트워크(이하 추론 네트워크)를 구성하였습니다. 임베딩 네트워크에는 오직 텍스트 데이터만 입력으로 사용되며 4215개의 카테고리 분류를 수행합니다. 해당 과정을 통해 학습된 네트워크의 중간 레이어를 추론 네트워크의 입력으로 사용하였습니다. 추론 네트워크에서는 임베딩 네트워크를 통해 임베딩된 벡터와 img_feat 을 입력으로 하여, 4215 개의 카테고리 분류를 수행합니다.

#### - 모델 사이즈가 최종 탈락 원인 :dango:
* 최종 제출 모델의 **임베딩 네트워크 크기는 5 GB 정도**이며, **추론 네트워크는 900 MB 정도**입니다.임베딩 과 추론 네트워크를 나눈 후, 임베딩 네트워크의 존재를 잊고 있었습니다. 추론 모델의 크기만 생각하다 조건을 충족한 것으로 착각하여 최종 제출 모델의 사이즈가 1 GB 를 넘었습니다. 임베딩 네트워크에서 학습 이후 사용되지 않는 부분을 제거하고, 추론 네트워크를 포함하여 하이퍼 파라미터의 크기를 낮추면 충분히 1 GB 이하로 모델을 제출할 것으로 예상 되었지만, 시간 및 컴퓨팅 리소스가 부족하여 추가 수정 기간에 이를 완료하지 못하였습니다.


### :octopus::octopus: 라벨이 없는(라벨이 -1인) 데이터 처리 :lollipop::lollipop:
텍스트 데이터 처리 와 네트워크 구조를 수정하며 1월이 다가왔습니다. 대회가 한달 남은 시점에서 어떻게하면 점수를 올릴 수 있을까 궁리하던 차에 홈페이지에 올라온 다음 글이 눈에 띄었습니다. 

> 모든 샘플은 대/중분류 값이 존재하지만 소나 세분류 값은 없을 수도 있습니다. 예측 결과 파일을 제출할 때는 이점을 무시하고 모든 분류에 대해 예측해야합니다. 하지만 채점시에 실제로 존재하지 않는 분류 체계에 대해서는 점수에 영향을 주지 않습니다. 예를 들어 **대/중/소까지 분류 값이 존재하는 샘플에 대해서는 세 분류에 대해 어떤 값을 예측하더라도 점수에는 영향을 주지 않습니다.**

그리고 네트워크의 정확도를 낮추는 주요 원인 중 하나는 **s/d 카테고리가 -1(존재하지 않는 카테고리)로 예측되는 경우**였습니다. -1 인 라벨은 s/d 카테고리에만 존재합니다. 따라서 저희는 다음 표에서와 같이 네트워크가 s/d 를 -1로 예측한 경우 오히려 점수가 깎일 수 있으므로, 이 라벨을 존재하는 다른 라벨로 치환하고자 하였습니다. 

|       | -1 로 예측한 경우  | -1 외의 값으로 예측한 경우 |
| :----- | :--------------: | :---------------------: |
| 라벨이 -1 인 경우  | 점수 영향 X  | 점수 영향 X |
| 라벨이 -1 이 아닌 경우  |  점수 영향 O  | - |

* 저희 팀은 서로 다른 두 가지 관점에서 이 과정을 진행하였습니다. 네트워크 모델(예측값)에 대해 의존성이 있는 처리와 없는 처리 두 가지입니다. 


#### - 네트워크 모델에 의존성 있는 후처리 :lollipop:
* 네트워크 모델의 출력 중 b/m/s/d 에 대한 예측 값을 b’/m’/s’/d’ 이라고 한다면, 우리가 원하는 것은 다음 수식과 같습니다. s’/d’ 의 값이 -1 이고, 주어진 b’/m’에 대해 가장 높은 빈도수로 존재하는 -1 이 아닌 라벨 s/d 를 새로운 s’/ d’ 으로 대체하는 것 입니다. 이러한 확률 분포를 만들기 위해 학습 데이터를 바탕으로 확률을 계산합니다.


<p align="center"><img src="/img/eq1.png" width="300"></p>

* 이 과정은 네트워크 모델의 예측 결과에 대한 분포가 고려된 처리지만, 예측 모델이 바뀔 때마다 새롭게 다시 계산해야한다는 단점이 있습니다. 학습 데이터의 양이 굉장히 많기 때문에, 모델이 변경될 때 이를 반영하기위해 많은 시간이 걸립니다.

#### - 네트워크 모델에 의존성 없는 후처리 :lollipop:
* 네트워크 모델에의한 예측 값인 b’/m’ 이 원래의 b/m 을 잘 대변한다고 가정하면, b’/m’ 을 b/m 으로 생각한다면, 다음의 수식을 통해서 s’/d’ 이 -1 일 때, 새로운 s’/d’ 인 s*/d* 을 정의할 수 있습니다. 

<p align="center"><img src="/img/eq2.png" width="250"></p>

* 이 경우 네트워크 모델에의한 예측 결과에 대한 분포가 고려 되지 않지 않으며, b’/m’ 이 틀리게되면 새로운 s’/d’ 도 틀리게될 가능성이 높습니다. 하지만 네트워크 모델이 달라져도 추가적인 처리 없이 그대로 사용할 수 있고, 네트워크 모델의 b/m 에대한 예측 정확도가 높아진다면, 결과도 쉽게 좋아지게됩니다.


#### - 네트워크 모델 상위 n 개 라벨 고려 :lollipop:
* 예측 모델의 top_1 에 대한 **prediction confidence 가 낮은 경우들이 주로 틀리는 경우**가 많았습니다. 따라서, 상위 n 개를 종합적으로 고려하여 예측 모델을 만들어보고자 시도하였습니다. 수식은 다음과 같습니다. p^n 은 네트워크의 n 번째로 높은 confidence 값이며, 이 때 b 와 m 카테고리의 라벨과 동일한 인덱스가 1이되도록 하는 one-hot vector 로 나타낸 열 벡터가 b^n, m^n 입니다. 실제 구현 할 코드에서는 n=5 입니다. 

<p align="center"><img src="/img/eq3.png" width="230"></p>


#### - 상위1위 랭크의 정보가 비어있을 때는 하위 랭크에서 가져오는 치환방식 적용 :lollipop:
* '예측값에 대해 의존성 없는 후처리' 적용 후에도 **상당히 많은 정보가 비어 있었습니다(-1)**. 그런데 -1로 두기보다는 어떻게든 논리적으로 추측하여 정보를 채우는 것이 유리하기에 저희는 조금 다른 방법을 고안했습니다. 기존의 네트워크 모델은 confidence가 1위인 카테고리 세트를 output으로 내놓습니다. 그러나 만약 모델의 최종 output이 될 confidence 1순위의 정보가 완전히 채워져 있지 않다면, 나머지 하위 confidence 순위에 있는 카테고리에서 정보를 가져와서 채우는 방식을 적용해볼 수 있습니다. 수식도로 표현하자면 아래와 같습니다.

<p align="center"><img src="/img/fig7.png" width="800"></p> 

* 순차적으로 하위 2, 3, 4, 5위에서 카테고리 정보를 가져오며, 만약 위의 예시에서 나타난 것처럼 세부 카테고리(D)가 모두 비어있다면 세부 카테고리는 그대로 비워둔 채로 남겨뒀습니다. Dev set에 대하여 적용해본 결과, 17% 비어있던 s 카테고리가 거의 전부 채워졌고, 카테고리 전부 채워져 있는 경우가 기존 12.4%에서 33.0%로 약 20%포인트가 증가했습니다. 점수의 경우에도 **1.050263** 에서 **1.052212**으로 크게 올랐습니다.

### :octopus::octopus: 앙상블 처리
* s/d 카테고리가 -1 로 예측되는 경우 위 두 가지 후처리 방식(네트워크 모델 의존성/비의존성 처리)을 각각 적용해보았더니 두 과정 모두 유사하게 높은 성능 개선 효과를 얻었습니다. 

* 두 과정은 서로 다른 팀원에의해 독립적으로 진행했었습니다. 두 모델 모두 성능이 좋게 나왔기에, 우리는 두 모델을 적절히 섞어 더 나은 모델을 만들고자 하였습니다. 

* 실제 구현 상의 문제로 '네트워크 모델에 의존성 없는 후처리'는 적용 후에도 s’/d’ 이 -1 인 경우가 종종 존재했습니다. 하지만 정확도는 '네트워크 모델에 의존성 있는 후처리' 적용 후보다 높았습니다. 따라서, '네트워크 모델에 의존성 없는 처리' 후에도 -1 로 나오는 s’/d’ 에 대해서는 '네트워크 모델에 의존성 있는 후처리' 모델의 결과로 대치시켰습니다.

## :whale: 스코어 개선 과정 요약
다음 그림은 각 처리 과정에서 달성된 대략적인 퍼블릭 스코어를 보여줍니다.

<p align="center"><img src="/img/score_summary.png" width="700"></p>


## :whale: 후기
* 텍스트 데이터 처리를 처음 다루어봐서, 많은 리서치를 통해 다양한 적용을 수행했습니다. 후처리에 투자한 시간보다도 토큰화 및 임베딩에 많은 시간을 들였지만, 최종적으로 점수를 높이지 못해서 반영되지는 않았습니다. 

* 다른 상위 팀들의 모델을 보면서, 처음 보는 방법도 있었고, 생각만 하고 적용하지 못했던 것도 발견할 수 있었습니다. 

* 유사 대회가 있다면, 다음에는 적합한 모델 적용 및 개선에 더 집중할 수 있을 것 같습니다. 

* 카카오 베이스라인 코드를 포함하여 최종 제출 팀분들의 코드를 보며 많은 공부가 되었습니다. 

* 팀원 모두의 백그라운드가 달랐습니다. 서로가 다루는 프로그래밍 언어와 배경 지식이 서로 달랐지만, 데이터에 대한 분석 및 이를 바탕으로 모델을 개선하는 것은 저희 팀원 모두 공통적으로 할 수 있는 일이었습니다. 그리고 이번 쇼핑 카테고리 분류 대회 참여 경험을 통해 팀원들과 정말 즐거운 경험을 하게된 것 같습니다.

## :whale: 대회 또는 서비스의 개선 요구 사항
* 제공 데이터 용량이 크다고 할 수 있는데, 로컬 브라우저 환경에서만 데이터를 받을 수 밖에 없다보니 초기에 데이터 접근에 어려움이 있었습니다. 브라우저를 이용할 수 없는 환경에서도 데이터를 받을 수 있게된다면, 데이터에 대한 접근성을 높여서 많은 분들이 쉽게 데이터 분석에 접근할 수 있을 것 같습니다.
